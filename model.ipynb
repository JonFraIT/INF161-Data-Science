{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23aef5ab",
   "metadata": {},
   "source": [
    "# Modelling and prediction\n",
    "\n",
    "- Input: prepared data\n",
    "- Output: machine learning model, expected generalisation RMSE\n",
    "- Features: This system takes the prepared dataframe and builds a machine learning model\n",
    "for predicting number of cycles. Model selection, feature selection and handling missing data\n",
    "are important parts of this system. You should evaluate at least 3 fundamentally different\n",
    "modelling approaches before selecting the final model. We evaluate the performance of the\n",
    "system by comparing the predicted number of cycles with the known number of cycles on\n",
    "a validation/test data set. Specifically, the system should be evaluated with the root mean\n",
    "squared error (RMSE) of predictions. The system should report the expected generalisation RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb2c78",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### Modules used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380abe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso, SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6139284",
   "metadata": {},
   "source": [
    "###  Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22b0aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(('clean_data/training_set.csv'))\n",
    "validation_set = pd.read_csv(('clean_data/validation_set.csv'))\n",
    "test_set = pd.read_csv(('clean_data/test_set.csv'))\n",
    "toPredict_2022 = pd.read_csv(('clean_data/toPredict_2022.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7c3e1",
   "metadata": {},
   "source": [
    "## Handle missing values\n",
    "\n",
    "At first I thought about using the ffill() method that replace a NaN value with the value in the row above. However, a problem became apparent with that approach when I was scimming through the data manually. The problem is when its multiple rows in a row with missing data. Weather data is very time dependent and I therefore ffill() is not sufficient. A better approach would be to replace NaN values with the value for that hour the day before. \n",
    "\n",
    "A couple more things I will take care of is setting date and time as index again and change 'Hour', 'isWeekday', 'Month' to object type because I want to treat them like objects and not int. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d484f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleNullValues(dataset):\n",
    "    dataset = dataset.set_index('Dato + Tid') # Set date and time as index\n",
    "    dataset.index = pd.to_datetime(dataset.index,format=\"%Y-%m-%d %H:%M:%S\") # Change date and time to datetime\n",
    "    dataset = dataset.groupby(dataset.index.hour).fillna(method='ffill') # Fill NaN with value same hour the day before\n",
    "    dataset[['Hour', 'isWeekday', 'Month']] = dataset[['Hour', 'isWeekday', 'Month']].astype('object') # Change to object type\n",
    "    return dataset\n",
    "\n",
    "training_set = handleNullValues(training_set)\n",
    "validation_set = handleNullValues(validation_set)\n",
    "test_set = handleNullValues(test_set)\n",
    "toPredict_2022 = handleNullValues(toPredict_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4548ea3",
   "metadata": {},
   "source": [
    "## Choosing a model\n",
    "\n",
    "First I will split each dataset into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a287925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitSet(set):\n",
    "    X = set.drop(['Volum'], axis=1) # Features\n",
    "    y = set.Volum                   # Target\n",
    "    return X,y\n",
    "\n",
    "X_train, y_train = splitSet(training_set)\n",
    "X_validation, y_validation = splitSet(validation_set)\n",
    "X_test, y_test = splitSet(test_set)\n",
    "X_toPredict, y_toPredict = splitSet(toPredict_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99d1ed",
   "metadata": {},
   "source": [
    "### Baseline model\n",
    "\n",
    "The problem is a regression problem and I will therefore use regression models.\n",
    "I will use DummyRegressor from sklearn for the baseline model to get a general idea of what to expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aff1792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyRegressor()\n",
      "RMSE on test data:  74 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy = DummyRegressor(strategy=\"mean\") # Create an instance of DummyRegressor\n",
    "\n",
    "dummy.fit(X_train, y_train) # Fit the model\n",
    "y_pred = dummy.predict(X_validation) # Make a prediction\n",
    "MSE = mean_squared_error(y_validation, y_pred) # Calculate MSE\n",
    "RMSE = np.sqrt(MSE) # Root of MSE\n",
    "    \n",
    "print(dummy)\n",
    "print('RMSE on test data: ', round(RMSE), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d9324",
   "metadata": {},
   "source": [
    "RMSE is quite high for what im predicting. Id expect to be able to reduce it considerably. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd470e9c",
   "metadata": {},
   "source": [
    "### Preprocess \n",
    "\n",
    "To improve the model I must make the data I feed it efficient and accurate. For the preprocess im going to split the features into categorical and numeric features. For the categorical features I will use OneHotEncoder. This will convert the data into binary features that is One Hot Encoded, which means that if a feature is represented by that column it receives a 1, otherwise a 0. For the numeric features I will use StandardScaler. This standardizes a feature by subtracting the mean and then scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f12ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_train):\n",
    "    categorical_features = ['Hour', 'isWeekday', 'Month'] # categorical features\n",
    "    numeric_features = ['Globalstraling', 'Solskinstid', 'Lufttemperatur'] # numeric features\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown=\"ignore\"), categorical_features), # use OneHotEncoder on categoric features\n",
    "    ('standard_scaler', StandardScaler(), numeric_features)]) # use StandardScaler on numeric features\n",
    "    return preprocessor\n",
    "\n",
    "X_train_preprocessed = preprocess(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77e7f3",
   "metadata": {},
   "source": [
    "### Comparing models\n",
    "\n",
    "Below I have chosen a handful of regression models to compare which is most suited. After the preprocess I expect them to perform alot better than the DummyRegressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "940391d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyRegressor()\n",
      "RMSE on validation data:  74 \n",
      "\n",
      "LinearRegression()\n",
      "RMSE on validation data:  48 \n",
      "\n",
      "ElasticNet()\n",
      "RMSE on validation data:  64 \n",
      "\n",
      "Lasso()\n",
      "RMSE on validation data:  52 \n",
      "\n",
      "Ridge(alpha=0.1)\n",
      "RMSE on validation data:  48 \n",
      "\n",
      "SGDRegressor()\n",
      "RMSE on validation data:  48 \n",
      "\n",
      "KNeighborsRegressor()\n",
      "RMSE on validation data:  36 \n",
      "\n",
      "RandomForestRegressor()\n",
      "RMSE on validation data:  36 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_models(model):\n",
    "    model_pipe = make_pipeline(X_train_preprocessed, model) # Create pipeline\n",
    "    model_pipe.fit(X_train, y_train) # Fit the pipeline\n",
    "    y_pred = model_pipe.predict(X_validation) # Make prediction on validation set\n",
    "    MSE = mean_squared_error(y_validation, y_pred) # Calculate MSE\n",
    "    RMSE = np.sqrt(MSE) # Root of MSE\n",
    "    \n",
    "    print(model)\n",
    "    print('RMSE on validation data: ', round(RMSE), '\\n')\n",
    "    \n",
    "# Models to test\n",
    "dummy = DummyRegressor(strategy=\"mean\")\n",
    "linR = LinearRegression()\n",
    "en = ElasticNet()\n",
    "la = Lasso()\n",
    "ri = Ridge(alpha=0.1)\n",
    "sgd = SGDRegressor()\n",
    "KNR = KNeighborsRegressor()\n",
    "RFR = RandomForestRegressor()\n",
    "\n",
    "regression_models = [dummy, linR, en, la, ri, sgd, KNR, RFR]\n",
    "\n",
    "for model in regression_models:\n",
    "    compare_models(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b726ab",
   "metadata": {},
   "source": [
    "As expected every model performed better. Especially two models stand out, KNeighborsRegressor and RandomForestRegressor, which performed twice as good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170df016",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "Moving forward I will only look at KNeighborsRegressor and RandomForestRegressor as they performed the best. To look at feature importance I will remove one feature at a time to see how the RMSE changes. Its hard to tell exactly how much each feature impacts the model. I think the numeric features, aka weatherdata, don't impact the model as much because the correlation to Volum was low. I believe the categorical features had more impact because I could see clear differences. For example difference in mean Volum for hours was significant. \n",
    "\n",
    "Theres for sure a better way to test this without re-writing the previous methods but its the result thats important here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f328457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE without feature \" Lufttemperatur \"\n",
      "\n",
      "KNeighborsRegressor()\n",
      "RMSE on validation data:  39 \n",
      "\n",
      "RandomForestRegressor()\n",
      "RMSE on validation data:  40 \n",
      "\n",
      "RMSE without feature \" Globalstraling \"\n",
      "\n",
      "KNeighborsRegressor()\n",
      "RMSE on validation data:  37 \n",
      "\n",
      "RandomForestRegressor()\n",
      "RMSE on validation data:  37 \n",
      "\n",
      "RMSE without feature \" Solskinstid \"\n",
      "\n",
      "KNeighborsRegressor()\n",
      "RMSE on validation data:  36 \n",
      "\n",
      "RandomForestRegressor()\n",
      "RMSE on validation data:  37 \n",
      "\n",
      "RMSE without feature \" Hour \"\n",
      "\n",
      "KNeighborsRegressor()\n",
      "RMSE on validation data:  57 \n",
      "\n",
      "RandomForestRegressor()\n",
      "RMSE on validation data:  59 \n",
      "\n",
      "RMSE without feature \" isWeekday \"\n",
      "\n",
      "KNeighborsRegressor()\n",
      "RMSE on validation data:  51 \n",
      "\n",
      "RandomForestRegressor()\n",
      "RMSE on validation data:  51 \n",
      "\n",
      "RMSE without feature \" Month \"\n",
      "\n",
      "KNeighborsRegressor()\n",
      "RMSE on validation data:  41 \n",
      "\n",
      "RandomForestRegressor()\n",
      "RMSE on validation data:  41 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess is the same as previous method except I remove one feature\n",
    "def check_feature_process(X_train, feature_to_drop):\n",
    "    numeric_features = ['Lufttemperatur','Globalstraling','Solskinstid']\n",
    "    try:\n",
    "        numeric_features.remove(feature_to_drop)\n",
    "    except:\n",
    "        pass\n",
    "    categorical_features = ['Hour', 'isWeekday', 'Month']\n",
    "    try:\n",
    "        categorical_features.remove(feature_to_drop)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ('standard_scaler', StandardScaler(), numeric_features)])\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def check_feature_compare(model):\n",
    "    check_feature_pipe = make_pipeline(check_feature_preprocess, model) # Create pipeline\n",
    "    check_feature_pipe.fit(X_train_check_feature, y_train) # Fit the pipeline\n",
    "    y_pred = check_feature_pipe.predict(X_validation_check_feature) # Make prediction on validation set\n",
    "    MSE = mean_squared_error(y_validation, y_pred) # Calculate MSE\n",
    "    RMSE = np.sqrt(MSE) # Root of MSE\n",
    "    \n",
    "    print(model)\n",
    "    print('RMSE on validation data: ', round(RMSE), '\\n')\n",
    "    \n",
    "\n",
    "KNR = KNeighborsRegressor()\n",
    "RFR = RandomForestRegressor()\n",
    "\n",
    "best_models = [KNR, RFR]\n",
    "\n",
    "features_to_test = ['Lufttemperatur','Globalstraling','Solskinstid', 'Hour', 'isWeekday', 'Month']\n",
    "\n",
    "for n_feature in features_to_test:\n",
    "    X_train_check_feature = X_train # Reset dataset for each feature to only remove one at a time\n",
    "    X_validation_check_feature = X_validation # Reset dataset for each feature to only remove one at a time\n",
    "    \n",
    "    X_train_check_feature = X_train.drop([n_feature], axis=1) # Drop specified feature\n",
    "    X_validation_check_feature = X_validation.drop([n_feature], axis=1) # Drop specified feature\n",
    "    \n",
    "    print('RMSE without feature \"', n_feature,'\"\\n')\n",
    "    check_feature_preprocess = check_feature_process(X_train_check_feature, n_feature)\n",
    "    for model in best_models:\n",
    "        check_feature_compare(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a8cf76",
   "metadata": {},
   "source": [
    "As expected, the numeric features had low impact and the categorical features was most important. The feature 'Hour' being the top factor increasing RMSE by about 20 while removed. What did surprise me what just how little the weatherdata affected the model. Removing 'Solskinstid' didn't change anything and removing 'Globalstraling' only increased rmse by 1. Only numeric feature which makes a noticeable difference when removed is Lufttemperatur which increased RMSE by 3-4.\n",
    "\n",
    "Currently the models KNR and RFR are slow at making predictions. I will remove the features 'Solskinstid' and 'Globalstraling' and recalculate RMSE. This will make the model simpler and speed it up without losing too much accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bace15da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor()\n",
      "RMSE on validation data:  37 \n",
      "\n",
      "RandomForestRegressor()\n",
      "RMSE on validation data:  38 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_selected_features = X_train.drop(['Globalstraling','Solskinstid'], axis=1)\n",
    "X_validation_selected_features = X_validation.drop(['Globalstraling','Solskinstid'], axis=1)\n",
    "\n",
    "def preprocess_selected_features(X_train):\n",
    "    numeric_features = ['Lufttemperatur']\n",
    "    categorical_features = ['Hour', 'isWeekday', 'Month']\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ('standard_scaler', StandardScaler(), numeric_features)])\n",
    "    return preprocessor\n",
    "\n",
    "X_train_selected_preprocessed = preprocess_selected_features(X_train_selected_features)\n",
    "\n",
    "def compare_models(model):\n",
    "    model_pipe = make_pipeline(X_train_selected_preprocessed, model) # Create pipeline\n",
    "    model_pipe.fit(X_train_selected_features, y_train) # Fit the pipeline\n",
    "    y_pred = model_pipe.predict(X_validation_selected_features) # Make prediction on validation set\n",
    "    MSE = mean_squared_error(y_validation, y_pred) # Calculate MSE\n",
    "    RMSE = np.sqrt(MSE) # Root of MSE\n",
    "    \n",
    "    print(model)\n",
    "    print('RMSE on validation data: ', round(RMSE), '\\n')\n",
    "    \n",
    "    \n",
    "KNR = KNeighborsRegressor()\n",
    "RFR = RandomForestRegressor()\n",
    "\n",
    "regression_models = [KNR, RFR]\n",
    "\n",
    "for model in regression_models:\n",
    "    compare_models(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682fb04",
   "metadata": {},
   "source": [
    "Removing 'Globalstraling' and 'Solskinstid' only increased RMSE with KNR by 1. I will choose KNR as the best model because it now has the lowest RMSE and it makes predictions faster as well. Now that I have selected my model I will calculate RMSE on the test set and save the mode. I expect RMSE to be the same on the test set as the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "227b8664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor()\n",
      "RMSE on test data:  27 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = make_pipeline(X_train_selected_preprocessed, KNR) # Create pipeline\n",
    "best_model.fit(X_train_selected_features, y_train) # Fit the pipeline\n",
    "\n",
    "X_test_selected_features = X_test.drop(['Globalstraling','Solskinstid'], axis=1) # Drop bad features\n",
    "\n",
    "y_pred = best_model.predict(X_test_selected_features) # Make prediction on validation set\n",
    "MSE = mean_squared_error(y_test, y_pred) # Calculate MSE on test data\n",
    "RMSE = np.sqrt(MSE) # Root of MSE\n",
    "    \n",
    "print(KNR)\n",
    "print('RMSE on test data: ', round(RMSE), '\\n')\n",
    "\n",
    "pickle.dump(best_model, open('model.pkl', 'wb')) # Save the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc1db7",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "I will then use this model to predict Volum for 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61b67d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "volum_prediction = best_model.predict(X_toPredict) # Make prediction\n",
    "toPredict_2022.Volum = volum_prediction # Fill Volum column with predictions\n",
    "toPredict_2022.Volum = toPredict_2022.Volum.round() # Round to whole numbers\n",
    "toPredict_2022.to_csv('predictions.csv') # Save as csv file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
